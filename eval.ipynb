{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "CV = 5\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train_tfidf, y_train, scoring='accuracy')\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51277f",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e1be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737aacb",
   "metadata": {},
   "source": [
    "### Baseline Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a90728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels,predictions):\n",
    "    \"\"\"Plots the confusion matrix\n",
    "    Arguments:\n",
    "    labels: array-like of shape (n_samples,)\n",
    "    predictions: array-like of shape (n_samples,)\n",
    "    Returns\n",
    "    -------\n",
    "    plot\n",
    "        plots the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(labels,predictions)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb1f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = df_keyword_matching(df)\n",
    "def baselineAccuracy(predictions, df):\n",
    "    \"\"\"Calculates the accuracy\n",
    "        Arguments:\n",
    "        predictions: list\n",
    "        df: a pandas dataframe that contains a column named text with utterances.\n",
    "        Returns\n",
    "        -------\n",
    "    Returns:\n",
    "        Returns the accuracy\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        \n",
    "        if(predictions[i].lower() == df.loc[i,'label'].lower()):\n",
    "            count += 1\n",
    "    return \"Accuracy: \"+str(round(count / len(predictions)*100,1))+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5706c987",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baselineAccuracy(\u001b[43mpredictions\u001b[49m, df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "baselineAccuracy(predictions, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f260e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def metrics_overview(labels, predictions):\n",
    "        \"\"\"Prints metrics\n",
    "        Arguments:\n",
    "        labels: array-like of shape (n_samples,)\n",
    "        predictions: array-like of shape (n_samples,)\n",
    "        \n",
    "        Prints different metrics related to the confusion matrix.\n",
    "        \"\"\"\n",
    "        edges_confusion_matrix = confusion_matrix(labels,predictions)\n",
    "\n",
    "        FP = edges_confusion_matrix.sum(axis=0) - np.diag(edges_confusion_matrix)  \n",
    "        \n",
    "        FN = edges_confusion_matrix.sum(axis=1) - np.diag(edges_confusion_matrix)\n",
    "        \n",
    "        TP = np.diag(edges_confusion_matrix)\n",
    "        \n",
    "        TN = edges_confusion_matrix.sum() - (FP + FN + TP)\n",
    "        \n",
    "        \n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = TP/(TP+FN)\n",
    "        print('TPR',TPR)\n",
    "        print('Average TPR',np.average(TPR))\n",
    "        print('_______________________________')\n",
    "        # Specificity or true negative rate\n",
    "        TNR = TN/(TN+FP)\n",
    "        print('TNR',TNR)\n",
    "        print('Average TNR',np.average(TNR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Precision or positive predictive value\n",
    "        PPV = TP/(TP+FP)\n",
    "        print('PPV',PPV)\n",
    "        print('Average PPV',np.average(PPV))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Negative predictive value\n",
    "        NPV = TN/(TN+FN)\n",
    "        print('NPV',NPV)\n",
    "        print('Average NPV',np.average(NPV))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        print('FPR',FPR)\n",
    "        print('Average FPR',np.average(FPR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # False negative rate\n",
    "        FNR = FN/(TP+FN)\n",
    "        print('FNR',FNR)\n",
    "        print('Average FNR',np.average(FNR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # False discovery rate\n",
    "        FDR = FP/(TP+FP)\n",
    "        print('FDR',FDR)\n",
    "        print('Average FDR',np.average(FDR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Overall accuracy\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        print('ACC',ACC)\n",
    "        print('Average ACC',np.average(ACC))\n",
    "        print('_______________________________')\n",
    "\n",
    "        F1 = 2*((PPV*TPR)/(PPV+TPR))\n",
    "        F1 = F1[~np.isnan(F1)]\n",
    "        print('F1',F1)\n",
    "        print('Average F1',np.average(F1))\n",
    "        print('_______________________________')\n",
    "        print((FP+FN)/(TP+FP+FN+TN))\n",
    "        \n",
    "metrics_overview(df['label'],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f32e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Proper Models (Random Forest, Multinomial NB, Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6784b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names= df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a847537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Greens', fmt='d',\n",
    "            xticklabels=label_dict.label.values, \n",
    "            yticklabels=label_dict.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e21bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(df['label'], predictions)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Greens', fmt='d',\n",
    "            xticklabels=label_dict.label.values, \n",
    "            yticklabels=label_dict.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ba55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
