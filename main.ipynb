{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb1a302",
   "metadata": {},
   "source": [
    "# WIP - Restaurant Recommendation Dialog System\n",
    "\n",
    "## Members:\n",
    "- Karpiński, R.R. (Rafał)\n",
    "- Pavan, L. (Lorenzo)\n",
    "- Rodrigues Luchetti, G.L. (Gustavo)\n",
    "- Teunissen, N.D. (Niels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002f13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    \"\"\"\n",
    "    Getting dataset from .dat file\n",
    "    args: dataset file path\n",
    "    return: DataFrame of dataset\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(lambda x: x.rstrip(\"\\n\").split(\" \", 1), data))\n",
    "    df = pd.DataFrame(np.array(data), columns = ['label', 'text'])    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d021b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset\n",
    "    args: DataFrame dataset\n",
    "    return: dataset\n",
    "    \"\"\"\n",
    "    df_proc = df.copy()\n",
    "    print(\"before removing 'noise': \")\n",
    "    print(df_proc.describe())\n",
    "\n",
    "    # if text contains \"noise\" or \"tv_noise\", remove it\n",
    "    for index, row in df_proc.iterrows():\n",
    "        if \"tv_noise\" in row[1]:\n",
    "            row[1] = row[1].replace('tv_noise', '')\n",
    "        elif \"noise\" in row[1]:\n",
    "            row[1] = row[1].replace('noise', '')\n",
    "        row[1] = row[1].strip()\n",
    "\n",
    "    # if that makes the column empty, remove the row\n",
    "    for i in range(len(df_proc.index)):\n",
    "        text = df['text'][i]\n",
    "        if not text:\n",
    "            df_proc = df.drop([i])\n",
    "\n",
    "    print(\"\\nafter: \")\n",
    "    print(df_proc.describe())\n",
    "    \n",
    "    # making label dict (turning labels into numbers)\n",
    "    df['label_id'] = df['label'].factorize()[0]\n",
    "    label_dict = df[['label','label_id']].drop_duplicates().set_index('label_id')\n",
    "    \n",
    "    return df_proc, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66315d4b",
   "metadata": {},
   "source": [
    "## Building Baseline Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae80c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class(df):\n",
    "    majority_class = df['label'].mode().to_string(index = False)\n",
    "    print(f\"Majority class is '{majority_class}'\")\n",
    "    return majority_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fe593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we remove null completely?\n",
    "keyword_dict = {\n",
    "    \"inform\": \"\\blooking for\\b|\\bdont care\\b|\\bdoesnt matter\\b|\\bexpensive\\b|\\bcheap\\b|\\bmoderate\\b|\\bi need\\b|\\bi want\\b|\\bfood\\b|\\bnorth\\b\",\n",
    "    \"confirm\": \"\\bdoes it\\b|\\bis it\\b|\\bdo they\\b|\\bis that\\b|\\bis there\\b\",\n",
    "    \"affirm\": \"\\byes\\b|\\byeah\\b|\\bcorrect\\b\",\n",
    "    \"request\": \"\\bwhat is\\b|\\bwhats\\b|\\bmay i\\b|\\bcould i\\b|\\bwhat\\b|\\bprice range\\b|\\bpost code\\b|\\btype of\\b|\\baddress\\b|\\bphone number\\b|\\bcan i\\b|\\bcould i\\b|\\bcould you\\b|\\bdo you\\b|\\bi want+.address\\b|\\bi want+.phone\\b|\\bi would\\b|\\bwhere is\\b\",\n",
    "    \"thankyou\": \"\\bthank you\\b\",\n",
    "    \"bye\": \"\\bgoodbye\\b|\\bbye\\b\",\n",
    "    \"reqalts\": \"\\bhow about\\b|\\bwhat about\\b|\\banything else\\b|\\bare there\\b|\\bis there\\b|\\bwhat else\\b\",\n",
    "    \"negate\": \"\\bno\\b|\\bnot\\b\",\n",
    "    \"hello\": \"\\bhello\\b\",\n",
    "    \"repeat\": \"\\brepeat\\b\",\n",
    "    \"ack\": \"\\bokay\\b|\\bkay\\b\",\n",
    "    \"restart\": \"\\bstart\\b\",\n",
    "    \"deny\": \"\\bdont\\b\",\n",
    "    \"reqmore\": \"\\bmore\\b\",\n",
    "    \"null\": \"_?_\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first baseline system - \"always assigns the majority class of in the data\"\n",
    "def single_majority_class(utterance):\n",
    "    \"\"\"\n",
    "    Classifies dialog based on the majority class label.\n",
    "    args: utterance (any string)\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    return majority_class      \n",
    "\n",
    "def df_majority_class(dataframe):\n",
    "    \"\"\"\n",
    "    Classifies dialog based on the majority class label.\n",
    "    args: pandas DataFrame that contains a column named text with utterances.\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        predictions.append(majority_class)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b486ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second baseline system - \"rule-based system based on keyword matching\"\n",
    "def single_keyword_matching(text):\n",
    "    \"\"\"\n",
    "    Rule-based prediction of a dialog act based on a phrase.\n",
    "    args: utterance (any string)\n",
    "    returns: Returns the predicted dialog act.\n",
    "    \"\"\"\n",
    "    label = \"inform\"\n",
    "    for key in keyword_dict:\n",
    "        if (re.search(keyword_dict[key], text)): # if we find one of our keywords on any given string\n",
    "            label = key\n",
    "            return\n",
    "    return label\n",
    "\n",
    "def df_keyword_matching(dataframe):\n",
    "    \"\"\"\n",
    "    Rule-based prediction of dialog acts based on a colletion of utterances.\n",
    "    args: DataFrame that contains a column named text with utterances.\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        text = df.loc[i, 'text']\n",
    "        predictions.append(single_keyword_matching(text))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25a7ff",
   "metadata": {},
   "source": [
    "## Building Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421d48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_model(method, df):\n",
    "    \"\"\"\n",
    "    Trains any method of classifier that fits the pre-processing.\n",
    "    args: model being used (and any parameters for said model), and a dataframe\n",
    "    returns: tuple of a trained, fitted model and a NLP vectorizer/transformer\n",
    "    \"\"\"\n",
    "    # X - independent features (excluding target variable).\n",
    "    # y - dependent variables (target we're looking to predict).\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label_id'], test_size=0.15, random_state=10\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        sublinear_tf=True, # scale the words frequency in logarithmic scale\n",
    "        min_df=5, # remove the words which has occurred in less than ‘min_df’ number of files\n",
    "        ngram_range=(1, 2), # don't know what role n-grams play in vectorisation\n",
    "        stop_words='english', # it removes stop words which are predefined in ‘english’.\n",
    "        lowercase=True # everything to lowercase\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "    labels = label_train\n",
    "\n",
    "    model = method\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "    y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "    return (model, tfidf)\n",
    "\n",
    "# shorthands for model training\n",
    "def train_logistic_regression_model(df):\n",
    "    return train_model(LogisticRegression(random_state=10, max_iter=400), df)\n",
    "\n",
    "def train_NB_classifier_model(df):\n",
    "    return train_model(MultinomialNB(), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef2415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(classifier):\n",
    "    \"\"\"Prepares the dataset, model and runs the bot\"\"\"\n",
    "    df = get_dataset('dialog_acts.dat')\n",
    "    df, label_dict = preprocess(df)\n",
    "    print('\\nDF after processing: ')\n",
    "    print(df.head(3))\n",
    "    majority_class = get_majority_class(df)\n",
    "    lr, lr_vectorizer = train_logistic_regression_model(df)\n",
    "    nb, nb_vectorizer = train_NB_classifier_model(df)\n",
    "    \n",
    "    bot(model, vectorizer, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615112e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing 'noise': \n",
      "         label                text\n",
      "count    25501               25501\n",
      "unique      15                5359\n",
      "top     inform  thank you good bye\n",
      "freq     10160                2565\n",
      "\n",
      "after: \n",
      "         label                text\n",
      "count    25501               25501\n",
      "unique      15                5289\n",
      "top     inform  thank you good bye\n",
      "freq     10160                2569\n",
      "DF after processing: \n",
      "    label                                               text\n",
      "0  inform  im looking for a moderately priced restaurant ...\n",
      "1  inform                                   any part of town\n",
      "2  inform                                        bistro food\n",
      "Majority class is 'inform'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [21], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      7\u001b[0m majority_class \u001b[38;5;241m=\u001b[39m get_majority_class(df)\n\u001b[0;32m----> 8\u001b[0m lr, lr_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_logistic_regression_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m nb, nb_vectorizer \u001b[38;5;241m=\u001b[39m train_NB_classifier_model(df)\n\u001b[1;32m     11\u001b[0m bot(model, vectorizer, label_dict)\n",
      "Cell \u001b[0;32mIn [19], line 40\u001b[0m, in \u001b[0;36mtrain_logistic_regression_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_logistic_regression_model\u001b[39m(df):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [19], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(method, df)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mTrains any method of classifier that fits the pre-processing.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03margs: model being used (and any parameters for said model), and a dataframe\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mreturns: tuple of a trained, fitted model and a NLP vectorizer/transformer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# X - independent features (excluding target variable).\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# y - dependent variables (target we're looking to predict).\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m---> 16\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m     20\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# scale the words frequency in logarithmic scale\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;66;03m# remove the words which has occurred in less than ‘min_df’ number of files\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# everything to lowercase\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_id'"
     ]
    }
   ],
   "source": [
    "main(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
