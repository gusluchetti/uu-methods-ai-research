{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb1a302",
   "metadata": {},
   "source": [
    "# WIP - Restaurant Recommendation Dialog System\n",
    "\n",
    "## Members:\n",
    "- Karpiński, R.R. (Rafał)\n",
    "- Pavan, L. (Lorenzo)\n",
    "- Rodrigues Luchetti, G.L. (Gustavo)\n",
    "- Teunissen, N.D. (Niels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002f13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    \"\"\"\n",
    "      args: dataset file path\n",
    "      return: DataFrame of dataset\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(lambda x: x.rstrip(\"\\n\").split(\" \", 1), data))\n",
    "    df = pd.DataFrame(np.array(data), columns = ['label', 'text'])    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d021b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset\n",
    "    args: DataFrame dataset\n",
    "    return: dataset\n",
    "    \"\"\"\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485147b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_factorize(df):\n",
    "    \"\"\"Does label factorization for a dataframe that has a column labeled 'label'\"\"\"\n",
    "    # making label dict (turning labels into numbers)\n",
    "    df['label_id'] = df['label'].factorize()[0]\n",
    "    label_dict = df[['label','label_id']].drop_duplicates().set_index('label_id')\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66315d4b",
   "metadata": {},
   "source": [
    "## Building Baseline Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae80c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class():\n",
    "    majority_class = df['label'].mode().to_string(index = False)\n",
    "    print(f\"Majority class is '{majority_class}' \")\n",
    "    \n",
    "    return majority_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fe593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we remove null completely?\n",
    "keyword_dict = {\n",
    "    \"inform\": \"\\blooking for\\b|\\bdont care\\b|\\bdoesnt matter\\b|\\bexpensive\\b|\\bcheap\\b|\\bmoderate\\b|\\bi need\\b|\\bi want\\b|\\bfood\\b|\\bnorth\\b\",\n",
    "    \"confirm\": \"\\bdoes it\\b|\\bis it\\b|\\bdo they\\b|\\bis that\\b|\\bis there\\b\",\n",
    "    \"affirm\": \"\\byes\\b|\\byeah\\b|\\bcorrect\\b\",\n",
    "    \"request\": \"\\bwhat is\\b|\\bwhats\\b|\\bmay i\\b|\\bcould i\\b|\\bwhat\\b|\\bprice range\\b|\\bpost code\\b|\\btype of\\b|\\baddress\\b|\\bphone number\\b|\\bcan i\\b|\\bcould i\\b|\\bcould you\\b|\\bdo you\\b|\\bi want+.address\\b|\\bi want+.phone\\b|\\bi would\\b|\\bwhere is\\b\",\n",
    "    \"thankyou\": \"\\bthank you\\b\",\n",
    "    \"bye\": \"\\bgoodbye\\b|\\bbye\\b\",\n",
    "    \"reqalts\": \"\\bhow about\\b|\\bwhat about\\b|\\banything else\\b|\\bare there\\b|\\bis there\\b|\\bwhat else\\b\",\n",
    "    \"negate\": \"\\bno\\b|\\bnot\\b\",\n",
    "    \"hello\": \"\\bhello\\b\",\n",
    "    \"repeat\": \"\\brepeat\\b\",\n",
    "    \"ack\": \"\\bokay\\b|\\bkay\\b\",\n",
    "    \"restart\": \"\\bstart\\b\",\n",
    "    \"deny\": \"\\bdont\\b\",\n",
    "    \"reqmore\": \"\\bmore\\b\",\n",
    "    \"null\": \"_?_\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b486ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_keyword_matching(text):\n",
    "    \"\"\"\n",
    "    Rule-based prediction of a dialog act based on a phrase.\n",
    "    args: utterance (any string)\n",
    "    returns: Returns the predicted dialog act.\n",
    "    \"\"\"\n",
    "    label = \"inform\"\n",
    "    for key in keyword_dict:\n",
    "        if (re.search(keyword_dict[key], text)): #if we find one of our keywords on any given string\n",
    "            label = key\n",
    "            return\n",
    "    return label\n",
    "\n",
    "def df_majority_class(dataframe):\n",
    "    \"\"\"\n",
    "    Classifies dialog based on the majority class label.\n",
    "    args: pandas DataFrame that contains a column named text with utterances.\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        predictions.append(majority_class)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56e78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_majority_class(utterance):\n",
    "    \"\"\"\n",
    "    Classifies dialog based on the majority class label.\n",
    "    args: utterance (any string)\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    return majority_class      \n",
    "\n",
    "def df_keyword_matching(dataframe):\n",
    "    \"\"\"\n",
    "    Rule-based prediction of dialog acts based on a colletion of utterances.\n",
    "    args: DataFrame that contains a column named text with utterances.\n",
    "    returns: list of predictions about the label (dialog act) of the utterances.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        text = df.loc[i, 'text']\n",
    "        predictions.append(single_keyword_matching(text))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25a7ff",
   "metadata": {},
   "source": [
    "## Building Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421d48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(method, df):\n",
    "    \"\"\"\n",
    "    Trains any method of classifier that fits the pre-processing.\n",
    "    args: model being used (and any parameters for said model), and a dataframe\n",
    "    returns: tuple of a trained, fitted model and a NLP vectorizer/transformer\n",
    "    \"\"\"\n",
    "    # X - independent features (excluding target variable).\n",
    "    # y - dependent variables (target we're looking to predict).\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label_id'], test_size=0.15, random_state=10\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        sublinear_tf=True, # scale the words frequency in logarithmic scale\n",
    "        min_df=5, # remove the words which has occurred in less than ‘min_df’ number of files\n",
    "        ngram_range=(1, 2), # don't know what role n-grams play in vectorisation\n",
    "        stop_words='english', # it removes stop words which are predefined in ‘english’.\n",
    "        lowercase=True # everything to lowercase\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "    labels = label_train\n",
    "\n",
    "    model = method\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "    y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "    return (model, tfidf)\n",
    "\n",
    "# shorthands for model training\n",
    "def train_logistic_regression_model(df):\n",
    "    return train_model(LogisticRegression(random_state=0, max_iter=400), df)\n",
    "\n",
    "def train_NB_classifier_model(df):\n",
    "    return train_model(MultinomialNB(), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b183f9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#CV = 5\n",
    "#entries = []\n",
    "#for model in models:\n",
    "#    model_name = model.__class__.__name__\n",
    "#    accuracies = cross_val_score(model, X_train_tfidf, y_train, scoring='accuracy')\n",
    "#    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#        entries.append((model_name, fold_idx, accuracy))\n",
    "#    \n",
    "#cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef2415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Prepares the dataset, model and runs the bot\"\"\"\n",
    "    df = get_dataset('dialog_acts.dat')\n",
    "    # df = preprocess(df)\n",
    "    label_dict = label_factorize(df)\n",
    "    majority_class = get_majority_class(df)\n",
    "    \n",
    "    model, vectorizer = train_logistic_regression_model(df)\n",
    "    # model, vectorizer = train_NB_classifier_model(df)\n",
    "    bot(model, vectorizer, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615112e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70fa92e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac27a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8c8b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7750b",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd11caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8ecda",
   "metadata": {},
   "source": [
    "### Baseline Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7120b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels,predictions):\n",
    "    \"\"\"Plots the confusion matrix\n",
    "    Arguments:\n",
    "    labels: array-like of shape (n_samples,)\n",
    "    predictions: array-like of shape (n_samples,)\n",
    "    Returns\n",
    "    -------\n",
    "    plot\n",
    "        plots the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(labels,predictions)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cad79d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m df_keyword_matching(\u001b[43mdf\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbaselineAccuracy\u001b[39m(predictions, df):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"Calculates the accuracy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Arguments:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        predictions: list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        Returns the accuracy\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = df_keyword_matching(df)\n",
    "def baselineAccuracy(predictions, df):\n",
    "    \"\"\"Calculates the accuracy\n",
    "        Arguments:\n",
    "        predictions: list\n",
    "        df: a pandas dataframe that contains a column named text with utterances.\n",
    "        Returns\n",
    "        -------\n",
    "    Returns:\n",
    "        Returns the accuracy\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        \n",
    "        if(predictions[i].lower() == df.loc[i,'label'].lower()):\n",
    "            count += 1\n",
    "    return \"Accuracy: \"+str(round(count / len(predictions)*100,1))+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineAccuracy(predictions, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e0080",
   "metadata": {},
   "outputs": [],
   "source": [
    " def metrics_overview(labels, predictions):\n",
    "        \"\"\"Prints metrics\n",
    "        Arguments:\n",
    "        labels: array-like of shape (n_samples,)\n",
    "        predictions: array-like of shape (n_samples,)\n",
    "        \n",
    "        Prints different metrics related to the confusion matrix.\n",
    "        \"\"\"\n",
    "        edges_confusion_matrix = confusion_matrix(labels,predictions)\n",
    "\n",
    "        FP = edges_confusion_matrix.sum(axis=0) - np.diag(edges_confusion_matrix)  \n",
    "        \n",
    "        FN = edges_confusion_matrix.sum(axis=1) - np.diag(edges_confusion_matrix)\n",
    "        \n",
    "        TP = np.diag(edges_confusion_matrix)\n",
    "        \n",
    "        TN = edges_confusion_matrix.sum() - (FP + FN + TP)\n",
    "        \n",
    "        \n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = TP/(TP+FN)\n",
    "        print('TPR',TPR)\n",
    "        print('Average TPR',np.average(TPR))\n",
    "        print('_______________________________')\n",
    "        # Specificity or true negative rate\n",
    "        TNR = TN/(TN+FP)\n",
    "        print('TNR',TNR)\n",
    "        print('Average TNR',np.average(TNR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Precision or positive predictive value\n",
    "        PPV = TP/(TP+FP)\n",
    "        print('PPV',PPV)\n",
    "        print('Average PPV',np.average(PPV))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Negative predictive value\n",
    "        NPV = TN/(TN+FN)\n",
    "        print('NPV',NPV)\n",
    "        print('Average NPV',np.average(NPV))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        print('FPR',FPR)\n",
    "        print('Average FPR',np.average(FPR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # False negative rate\n",
    "        FNR = FN/(TP+FN)\n",
    "        print('FNR',FNR)\n",
    "        print('Average FNR',np.average(FNR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # False discovery rate\n",
    "        FDR = FP/(TP+FP)\n",
    "        print('FDR',FDR)\n",
    "        print('Average FDR',np.average(FDR))\n",
    "        print('_______________________________')\n",
    "\n",
    "        # Overall accuracy\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        print('ACC',ACC)\n",
    "        print('Average ACC',np.average(ACC))\n",
    "        print('_______________________________')\n",
    "\n",
    "        F1 = 2*((PPV*TPR)/(PPV+TPR))\n",
    "        F1 = F1[~np.isnan(F1)]\n",
    "        print('F1',F1)\n",
    "        print('Average F1',np.average(F1))\n",
    "        print('_______________________________')\n",
    "        print((FP+FN)/(TP+FP+FN+TN))\n",
    "        \n",
    "metrics_overview(df['label'],predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77ac18",
   "metadata": {},
   "source": [
    "### Proper Models (Random Forest, Multinomial NB, Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e503763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names= df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Greens', fmt='d',\n",
    "            xticklabels=label_dict.label.values, \n",
    "            yticklabels=label_dict.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(df['label'], predictions)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Greens', fmt='d',\n",
    "            xticklabels=label_dict.label.values, \n",
    "            yticklabels=label_dict.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19712c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
